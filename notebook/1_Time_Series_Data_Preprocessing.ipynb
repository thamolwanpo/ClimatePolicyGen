{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "G05sBc0d529o",
   "metadata": {
    "id": "G05sBc0d529o"
   },
   "source": [
    "# Import Libraries and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "XSd1w81B5rBb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3181,
     "status": "ok",
     "timestamp": 1728715878997,
     "user": {
      "displayName": "Thamolwan Poopradubsil",
      "userId": "17420620460076926594"
     },
     "user_tz": -420
    },
    "id": "XSd1w81B5rBb",
    "outputId": "a73b4641-eae1-41d9-afcc-b82642f5fe08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and mount Google Drive\n",
    "from google.colab import drive\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Mount Google Drive to access datasets\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add project directory to system path for importing custom modules\n",
    "sys.path.append('[your path]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0teJfPaQ6ae6",
   "metadata": {
    "id": "0teJfPaQ6ae6"
   },
   "source": [
    "# Load Metadata for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466f8ecb-da6c-41e3-96a9-5b55a1d9c371",
   "metadata": {
    "executionInfo": {
     "elapsed": 21287,
     "status": "ok",
     "timestamp": 1728715901555,
     "user": {
      "displayName": "Thamolwan Poopradubsil",
      "userId": "17420620460076926594"
     },
     "user_tz": -420
    },
    "id": "466f8ecb-da6c-41e3-96a9-5b55a1d9c371",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load metadata from JSON files in the 'owid' directory\n",
    "import json\n",
    "\n",
    "# Define directory path for datasets\n",
    "dir_path = '[your path]'\n",
    "\n",
    "# Initialize dictionary for storing table names\n",
    "table_names = dict()\n",
    "\n",
    "# Traverse the directory and load metadata from JSON files\n",
    "for root, dirs, files in os.walk(os.path.join(dir_path, 'owid')):\n",
    "    for file in files:\n",
    "        if file.endswith('.meta.json'):\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                obj = json.load(f)\n",
    "\n",
    "            # Extract and store the title of the dataset\n",
    "            title = obj.get('title', obj.get('dataset').get('title'))\n",
    "            table_names[title] = os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5FjhDVTH6icn",
   "metadata": {
    "id": "5FjhDVTH6icn"
   },
   "source": [
    "# Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "KwNQ9QnlVyOw",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728715901555,
     "user": {
      "displayName": "Thamolwan Poopradubsil",
      "userId": "17420620460076926594"
     },
     "user_tz": -420
    },
    "id": "KwNQ9QnlVyOw"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for numerical operations\n",
    "import numpy as np\n",
    "from timeRec.data import *\n",
    "\n",
    "# Define a function to filter columns in the dataset\n",
    "def filter_columns(df, columns_to_keep, missing_threshold=0.3, correlation_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Filters columns in the dataset by removing non-numeric, constant, or highly correlated columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        columns_to_keep (list): List of columns to retain, e.g., 'country', 'year'.\n",
    "        missing_threshold (float): Threshold for missing values (default is 0.3).\n",
    "        correlation_threshold (float): Threshold for correlation (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe.\n",
    "    \"\"\"\n",
    "    # Drop unnecessary columns\n",
    "    df_filtered = df.drop(columns=columns_to_keep, errors='ignore')\n",
    "\n",
    "    # Select only numeric columns\n",
    "    df_filtered = df_filtered.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Drop constant columns\n",
    "    constant_columns = df_filtered.nunique()[df_filtered.nunique() == 1].index\n",
    "    df_filtered = df_filtered.drop(columns=constant_columns)\n",
    "\n",
    "    # Drop columns with too many missing values\n",
    "    df_filtered = df_filtered.dropna(thresh=len(df_filtered) * (1 - missing_threshold), axis=1)\n",
    "\n",
    "    # Drop highly correlated columns\n",
    "    correlation_matrix = df_filtered.corr().abs()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_features = [col for col in upper_triangle.columns if any(upper_triangle[col] > correlation_threshold)]\n",
    "    df_filtered = df_filtered.drop(columns=high_corr_features)\n",
    "\n",
    "    # Add back necessary columns (e.g., 'country')\n",
    "    df_filtered = pd.concat([df[columns_to_keep], df_filtered], axis=1)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdf1dc-b647-4a96-ad62-cdbebcb96c59",
   "metadata": {
    "id": "0dcdf1dc-b647-4a96-ad62-cdbebcb96c59",
    "tags": []
   },
   "source": [
    "# Load and Filter Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bc79c4-9a9b-4e25-bb34-eea99e0f313f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 12807,
     "status": "error",
     "timestamp": 1728715914360,
     "user": {
      "displayName": "Thamolwan Poopradubsil",
      "userId": "17420620460076926594"
     },
     "user_tz": -420
    },
    "id": "f7bc79c4-9a9b-4e25-bb34-eea99e0f313f",
    "outputId": "36ee97b8-ffab-4dd6-9f7c-75021251a221",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8eb2cdd8eeee>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Load time series data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtime_series_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_time_series_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total number of columns: {total_len_columns}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8eb2cdd8eeee>\u001b[0m in \u001b[0;36mload_time_series_data\u001b[0;34m(table_names, countries)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Standardize column names for country and location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36mputmask\u001b[0;34m(a, mask, values)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a function to load and filter time series data\n",
    "def load_time_series_data(table_names, countries):\n",
    "    \"\"\"\n",
    "    Loads and filters time series data based on the table names dictionary.\n",
    "\n",
    "    Args:\n",
    "        table_names (dict): Dictionary mapping dataset titles to paths.\n",
    "        countries (list): List of countries to filter by.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed time series datasets.\n",
    "    \"\"\"\n",
    "    global total_len_columns, total_year\n",
    "    time_series_datasets = {}\n",
    "\n",
    "    for title, path in table_names.items():\n",
    "        df = pd.read_csv(path.replace('meta.json', 'csv'))\n",
    "\n",
    "        # Standardize column names for country and location\n",
    "        if 'country' in df.columns and 'location' in df.columns:\n",
    "            df = df.drop(columns=['location'])\n",
    "        elif 'location' in df.columns and 'country' not in df.columns:\n",
    "            df.rename(columns={'location': 'country'}, inplace=True)\n",
    "\n",
    "        # Filter by year range and country\n",
    "        df = df[(df['year'] >= 1970) & (df['year'] <= 2024)]\n",
    "        df = filter_columns(df, ['country', 'year'], missing_threshold=0.2, correlation_threshold=0.7)\n",
    "        df = df[df['country'].isin(countries)]\n",
    "\n",
    "        # Update global counters for columns and years\n",
    "        total_len_columns += len(df.columns) - 2\n",
    "        total_year.update(df['year'].unique())\n",
    "\n",
    "        # Store the filtered DataFrame\n",
    "        time_series_datasets[title] = df\n",
    "\n",
    "    return time_series_datasets\n",
    "\n",
    "# Initialize global variables\n",
    "total_len_columns = 0\n",
    "total_year = set()\n",
    "\n",
    "# Load time series data\n",
    "time_series_datasets = load_time_series_data(table_names, countries)\n",
    "\n",
    "print(f\"Total number of columns: {total_len_columns}\")\n",
    "print(f\"Number of unique years: {len(total_year)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zsQ9p3i6-km",
   "metadata": {
    "id": "5zsQ9p3i6-km"
   },
   "source": [
    "# Merge and Process Time Series Data by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JM2z2flW667o",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1728715914361,
     "user": {
      "displayName": "Thamolwan Poopradubsil",
      "userId": "17420620460076926594"
     },
     "user_tz": -420
    },
    "id": "JM2z2flW667o"
   },
   "outputs": [],
   "source": [
    "# Import Dask for parallel processing\n",
    "import dask.dataframe as dd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define a function to impute missing data\n",
    "def impute_missing_data(df, method='linear'):\n",
    "    \"\"\"\n",
    "    Imputes missing data using specified method, excluding 'year' and 'country' columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with missing data.\n",
    "        method (str): Imputation method ('ffill', 'bfill', 'linear').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    cols_to_impute = df.columns.difference(['year', 'country'])\n",
    "    if method == 'ffill':\n",
    "        df[cols_to_impute] = df[cols_to_impute].ffill()\n",
    "    elif method == 'bfill':\n",
    "        df[cols_to_impute] = df[cols_to_impute].bfill()\n",
    "    elif method == 'linear':\n",
    "        df[cols_to_impute] = df[cols_to_impute].apply(\n",
    "            lambda col: col.interpolate(method='linear') if col.dtype in ['float64', 'float32', 'int64', 'int32'] else col\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Define a function to optimize data types for memory efficiency\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Optimizes data types for memory efficiency.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to optimize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized DataFrame.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col != 'country':\n",
    "            if df[col].dtype == 'float64':\n",
    "                df[col] = df[col].astype('float32')\n",
    "            elif df[col].dtype == 'int64':\n",
    "                df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "# Define year range from 1970 to 2024\n",
    "year_range = pd.DataFrame({'year': list(range(1970, 2025))})\n",
    "npartitions = 50\n",
    "\n",
    "# Process time series data for each country\n",
    "for country in tqdm(countries):\n",
    "    time_series = None\n",
    "    merge_count = 0\n",
    "\n",
    "    for title, dataset in time_series_datasets.items():\n",
    "        country_data = dataset[dataset['country'] == country]\n",
    "        if country_data.empty:\n",
    "            continue\n",
    "\n",
    "        optimized_data = optimize_dtypes(country_data)\n",
    "\n",
    "        if time_series is None:\n",
    "            time_series = dd.from_pandas(optimized_data, npartitions=npartitions)\n",
    "        else:\n",
    "            new_data = dd.from_pandas(optimized_data, npartitions=npartitions)\n",
    "\n",
    "            overlap_columns = set(time_series.columns).intersection(set(new_data.columns)) - {'country', 'year'}\n",
    "            if overlap_columns:\n",
    "                suffix = f'_right_{merge_count}'\n",
    "                new_data = new_data.rename(columns={col: col + suffix for col in overlap_columns})\n",
    "\n",
    "            time_series = dd.merge(time_series, new_data, on=['country', 'year'], how='outer')\n",
    "            time_series = time_series.persist()\n",
    "            merge_count += 1\n",
    "\n",
    "    if time_series is not None:\n",
    "        # Convert to pandas and merge with full year range\n",
    "        country_df = time_series.compute()\n",
    "        country_df = pd.merge(year_range, country_df, on='year', how='left')\n",
    "\n",
    "        # Remove duplicates and fill missing values\n",
    "        country_df['non_null_count'] = country_df.notnull().sum(axis=1)\n",
    "        country_df = country_df.sort_values('non_null_count', ascending=False).drop_duplicates(['country', 'year'], keep='first')\n",
    "        country_df = country_df.drop(columns=['non_null_count']).sort_values(by='year')\n",
    "\n",
    "        # Impute missing data and save the result\n",
    "        country_df = impute_missing_data(country_df, method='linear')\n",
    "        country_df = impute_missing_data(country_df, method='bfill')\n",
    "\n",
    "        # Save the processed country data\n",
    "        os.makedirs(os.path.join(dir_path, 'output'), exist_ok=True)\n",
    "        country_df.to_csv(os.path.join(dir_path, f'output/{country.replace(\" \", \"_\")}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amOfcPaI7Ppz",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1728715914361,
     "user": {
      "displayName": "Thamolwan Poopradubsil",
      "userId": "17420620460076926594"
     },
     "user_tz": -420
    },
    "id": "amOfcPaI7Ppz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
