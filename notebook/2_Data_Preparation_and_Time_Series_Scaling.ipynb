{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256809af-24ae-4012-a133-9e772c6508de",
   "metadata": {},
   "source": [
    "# Import Libraries and Suppress Warnings\n",
    "This cell imports necessary libraries and suppresses warnings for cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6cf97c-c581-41a3-81c0-38173435feeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d8c4f-c77e-4e85-918a-088c4a4eb4cf",
   "metadata": {},
   "source": [
    "# Load Train and Test Datasets\n",
    "This cell loads the train and test datasets, filters out rows in `train_df` where the `Document ID` is in `test_df`, and removes rows with missing or irrelevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ff824e-ab6a-4904-89a9-93e79aed0f78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  7195\n",
      "Number of test data:  100\n",
      "Number of training data minus testing set:  7095\n",
      "Number of training data after removing row with no `Last event in timeline`:  4704\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('../climate_laws/Document_Data_Download-2024-08-27.csv')\n",
    "test_df = pd.read_csv('csv/test.csv')\n",
    "print('Number of train data: ', len(train_df))\n",
    "print('Number of test data: ', len(test_df))\n",
    "\n",
    "# Filter train_df by removing rows where the Document ID is in test_df\n",
    "train_df = train_df[~train_df['Document ID'].isin(test_df['Document ID'])]\n",
    "print('Number of training data minus testing set: ', len(train_df))\n",
    "\n",
    "# Remove rows that have no 'Sector' or where 'Sector' is 'Other'\n",
    "train_df = train_df.dropna(subset='Sector')\n",
    "train_df = train_df[train_df['Sector'] != 'Other']\n",
    "\n",
    "# Remove rows that have no 'Last event in timeline'\n",
    "train_df = train_df.dropna(subset='Last event in timeline')\n",
    "print('Number of training data after removing row with no `Last event in timeline`: ', len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509a1eb-e0d8-44f6-94f5-affab2ea8144",
   "metadata": {},
   "source": [
    "# Load and Merge Region Data\n",
    "This cell loads region data and merges it with the `train_df` dataset on 'Geography ISO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004404b4-2e4b-43fe-bb7e-2962c991ffc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Document Title</th>\n",
       "      <th>Family ID</th>\n",
       "      <th>Family Title</th>\n",
       "      <th>Family Summary</th>\n",
       "      <th>Collection Title(s)</th>\n",
       "      <th>Collection Description(s)</th>\n",
       "      <th>Document Role</th>\n",
       "      <th>Document Variant</th>\n",
       "      <th>Document Content URL</th>\n",
       "      <th>...</th>\n",
       "      <th>Full timeline of events (dates)</th>\n",
       "      <th>Date Added to System</th>\n",
       "      <th>Last Modified on System</th>\n",
       "      <th>Internal Document ID</th>\n",
       "      <th>Internal Family ID</th>\n",
       "      <th>Internal Collection ID(s)</th>\n",
       "      <th>alpha-3</th>\n",
       "      <th>region</th>\n",
       "      <th>sub-region</th>\n",
       "      <th>intermediate-region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mandatory-climate-information-reporting-for-co...</td>\n",
       "      <td>Mandatory Climate Information Reporting for Co...</td>\n",
       "      <td>mandatory-climate-information-reporting-for-co...</td>\n",
       "      <td>Mandatory Climate Information Reporting for Co...</td>\n",
       "      <td>&lt;p&gt;This policy sets out a new regulatory schem...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Main</td>\n",
       "      <td>Original Language</td>\n",
       "      <td>https://assets.publishing.service.gov.uk/media...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>2024-08-15</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>CPR.document.i00000456.n0000</td>\n",
       "      <td>CPR.family.i00000455.n0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>federal-budget-2022-23_0bfb</td>\n",
       "      <td>Federal Budget 2022-23</td>\n",
       "      <td>federal-budget-2022-23_3513</td>\n",
       "      <td>Federal Budget 2022-23</td>\n",
       "      <td>&lt;p&gt;The Budget includes $1.3 billion of new inv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Main</td>\n",
       "      <td>Original Language</td>\n",
       "      <td>https://archive.budget.gov.au/2022-23-october/...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>2024-08-17</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>CPR.document.i00000471.n0000</td>\n",
       "      <td>CPR.family.i00000470.n0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Australia and New Zealand</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resolution-no-490-on-requirements-of-the-air-p...</td>\n",
       "      <td>Resolution No. 490 on requirements of the Air ...</td>\n",
       "      <td>resolution-no-490-on-requirements-of-the-air-p...</td>\n",
       "      <td>Resolution No. 490 on requirements of the Air ...</td>\n",
       "      <td>&lt;p&gt;This 2022 regulation sets conventional emis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information Webpage</td>\n",
       "      <td>Original Language</td>\n",
       "      <td>https://www.iea.org/policies/17008-resolution-...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2024-08-15</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>CPR.document.i00000459.n0000</td>\n",
       "      <td>CPR.family.i00000458.n0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Latin America and the Caribbean</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decree-no-2-22-03-amending-decree-no-2-10-320-...</td>\n",
       "      <td>Decree No. 2.22.03, amending Decree No. 2.10.3...</td>\n",
       "      <td>decree-no-2-22-03-amending-decree-no-2-10-320-...</td>\n",
       "      <td>Decree No. 2.22.03, amending Decree No. 2.10.3...</td>\n",
       "      <td>&lt;p&gt;Morocco’s government has approved a decree ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Press Release</td>\n",
       "      <td>Original Language</td>\n",
       "      <td>https://www.moroccoworldnews.com/2022/01/34652...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2024-08-18</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>CPR.document.i00000498.n0000</td>\n",
       "      <td>CPR.family.i00000497.n0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solar-house-program_33bb</td>\n",
       "      <td>Solar House Program</td>\n",
       "      <td>solar-house-program_87e6</td>\n",
       "      <td>Solar House Program</td>\n",
       "      <td>&lt;p&gt;This program sets up a subsidy system for h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Press Release</td>\n",
       "      <td>Original Language</td>\n",
       "      <td>https://energia.gob.cl/noticias/aysen-del-gene...</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>2024-08-18</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>CPR.document.i00000513.n0000</td>\n",
       "      <td>CPR.family.i00000512.n0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHL</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Latin America and the Caribbean</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Document ID  \\\n",
       "0  mandatory-climate-information-reporting-for-co...   \n",
       "1                        federal-budget-2022-23_0bfb   \n",
       "2  resolution-no-490-on-requirements-of-the-air-p...   \n",
       "3  decree-no-2-22-03-amending-decree-no-2-10-320-...   \n",
       "4                           solar-house-program_33bb   \n",
       "\n",
       "                                      Document Title  \\\n",
       "0  Mandatory Climate Information Reporting for Co...   \n",
       "1                             Federal Budget 2022-23   \n",
       "2  Resolution No. 490 on requirements of the Air ...   \n",
       "3  Decree No. 2.22.03, amending Decree No. 2.10.3...   \n",
       "4                                Solar House Program   \n",
       "\n",
       "                                           Family ID  \\\n",
       "0  mandatory-climate-information-reporting-for-co...   \n",
       "1                        federal-budget-2022-23_3513   \n",
       "2  resolution-no-490-on-requirements-of-the-air-p...   \n",
       "3  decree-no-2-22-03-amending-decree-no-2-10-320-...   \n",
       "4                           solar-house-program_87e6   \n",
       "\n",
       "                                        Family Title  \\\n",
       "0  Mandatory Climate Information Reporting for Co...   \n",
       "1                             Federal Budget 2022-23   \n",
       "2  Resolution No. 490 on requirements of the Air ...   \n",
       "3  Decree No. 2.22.03, amending Decree No. 2.10.3...   \n",
       "4                                Solar House Program   \n",
       "\n",
       "                                      Family Summary Collection Title(s)  \\\n",
       "0  <p>This policy sets out a new regulatory schem...                 NaN   \n",
       "1  <p>The Budget includes $1.3 billion of new inv...                 NaN   \n",
       "2  <p>This 2022 regulation sets conventional emis...                 NaN   \n",
       "3  <p>Morocco’s government has approved a decree ...                 NaN   \n",
       "4  <p>This program sets up a subsidy system for h...                 NaN   \n",
       "\n",
       "  Collection Description(s)        Document Role   Document Variant  \\\n",
       "0                       NaN                 Main  Original Language   \n",
       "1                       NaN                 Main  Original Language   \n",
       "2                       NaN  Information Webpage  Original Language   \n",
       "3                       NaN        Press Release  Original Language   \n",
       "4                       NaN        Press Release  Original Language   \n",
       "\n",
       "                                Document Content URL  ...  \\\n",
       "0  https://assets.publishing.service.gov.uk/media...  ...   \n",
       "1  https://archive.budget.gov.au/2022-23-october/...  ...   \n",
       "2  https://www.iea.org/policies/17008-resolution-...  ...   \n",
       "3  https://www.moroccoworldnews.com/2022/01/34652...  ...   \n",
       "4  https://energia.gob.cl/noticias/aysen-del-gene...  ...   \n",
       "\n",
       "  Full timeline of events (dates) Date Added to System  \\\n",
       "0                      2022-02-21           2024-08-15   \n",
       "1                      2022-10-01           2024-08-17   \n",
       "2                      2022-01-01           2024-08-15   \n",
       "3                      2022-01-01           2024-08-18   \n",
       "4                      2020-10-06           2024-08-18   \n",
       "\n",
       "  Last Modified on System          Internal Document ID  \\\n",
       "0              2024-08-19  CPR.document.i00000456.n0000   \n",
       "1              2024-08-19  CPR.document.i00000471.n0000   \n",
       "2              2024-08-19  CPR.document.i00000459.n0000   \n",
       "3              2024-08-19  CPR.document.i00000498.n0000   \n",
       "4              2024-08-19  CPR.document.i00000513.n0000   \n",
       "\n",
       "           Internal Family ID Internal Collection ID(s) alpha-3    region  \\\n",
       "0  CPR.family.i00000455.n0000                       NaN     GBR    Europe   \n",
       "1  CPR.family.i00000470.n0000                       NaN     AUS   Oceania   \n",
       "2  CPR.family.i00000458.n0000                       NaN     BRA  Americas   \n",
       "3  CPR.family.i00000497.n0000                       NaN     MAR    Africa   \n",
       "4  CPR.family.i00000512.n0000                       NaN     CHL  Americas   \n",
       "\n",
       "                        sub-region intermediate-region  \n",
       "0                  Northern Europe                 NaN  \n",
       "1        Australia and New Zealand                 NaN  \n",
       "2  Latin America and the Caribbean       South America  \n",
       "3                  Northern Africa                 NaN  \n",
       "4  Latin America and the Caribbean       South America  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load region data\n",
    "region = pd.read_csv('../retriever/region.csv')\n",
    "region = region[['alpha-3', 'region', 'sub-region', 'intermediate-region']]\n",
    "\n",
    "# Merge region data with train_df\n",
    "train_df = train_df.merge(region, left_on='Geography ISO', right_on='alpha-3', how='left')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfcd35-187d-4d15-8d89-bd66a71a1842",
   "metadata": {},
   "source": [
    "# Clean Data and Filter Based on Geography\n",
    "This cell cleans the 'Family Summary' field, splits 'Sector' values, and filters the dataset by geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bc741f-9de6-4df4-a8fe-74b2ebddb441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data (with countries):  4541\n",
      "Number of testing data:  100\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from timeRec.data import *\n",
    "\n",
    "# Function to clean 'Family Summary' and filter by 'Geography'\n",
    "def clean_family_summary_split_sector_and_filter_geography(df, countries):\n",
    "    # Function to remove HTML tags from 'Family Summary'\n",
    "    def remove_html_tags(text):\n",
    "        if isinstance(text, str):\n",
    "            return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        return \"\"  # Return empty string if not a string\n",
    "\n",
    "    # Clean 'Family Summary' column\n",
    "    df['Family Summary'] = df['Family Summary'].apply(remove_html_tags)\n",
    "\n",
    "    # Remove rows where 'Family Summary' is empty or whitespace\n",
    "    df = df[df['Family Summary'].str.strip() != \"\"]\n",
    "\n",
    "    # Split 'Sector' column into a list\n",
    "    df['Sector'] = df['Sector'].apply(lambda x: x.split(';') if isinstance(x, str) else [])\n",
    "\n",
    "    # Keep only rows where 'Geography' is in the list of countries\n",
    "    df = df[df['Geography'].isin(countries)]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean and filter both train and test datasets\n",
    "train_df = clean_family_summary_split_sector_and_filter_geography(train_df, countries)\n",
    "test_df = clean_family_summary_split_sector_and_filter_geography(test_df, countries)\n",
    "\n",
    "print('Number of training data (with countries): ', len(train_df))\n",
    "print('Number of testing data: ', len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8d260-f193-45f4-b07a-1f9d211243b8",
   "metadata": {},
   "source": [
    "# Read, Concatenate, and Scale Time Series Data\n",
    "This cell reads, concatenates, and scales time series data, saving both the concatenated and scaled data to CSV files, and saving the scaler for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2021b397-1685-4c36-b344-18656b8f0721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time series records:  9790\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Function to read and scale time series data\n",
    "def read_add_missing_concat_and_scale(folder_path, output_file, scaled_output_file, scaler_output_file):\n",
    "    # List all CSV files in the folder\n",
    "    files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Initialize lists to hold DataFrames and unique columns\n",
    "    dataframes = []\n",
    "    all_columns = set()\n",
    "\n",
    "    # Read each file and track all unique columns\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append((file, df))  \n",
    "        all_columns.update(df.columns)\n",
    "\n",
    "    all_columns = list(all_columns)\n",
    "\n",
    "    # Align all DataFrames by adding missing columns (filling NaNs with 0)\n",
    "    aligned_dataframes = []\n",
    "    for file, df in dataframes:\n",
    "        missing_columns = [col for col in all_columns if col not in df.columns]\n",
    "        for col in missing_columns:\n",
    "            df[col] = 0\n",
    "        df = df[sorted(df.columns)]  # Sort columns alphabetically\n",
    "        aligned_dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_df = pd.concat(aligned_dataframes, axis=0)\n",
    "    final_df = final_df.fillna(0)\n",
    "\n",
    "    # Save the concatenated data before scaling\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Scale numeric columns using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    columns_to_exclude = ['country', 'year']\n",
    "    numeric_columns = final_df.select_dtypes(include=['number']).columns.difference(columns_to_exclude)\n",
    "    scaled_data = scaler.fit_transform(final_df[numeric_columns])\n",
    "\n",
    "    # Save the scaler for future use\n",
    "    joblib.dump(scaler, scaler_output_file)\n",
    "\n",
    "    # Replace numeric columns with scaled data\n",
    "    final_df[numeric_columns] = pd.DataFrame(scaled_data, columns=numeric_columns)\n",
    "\n",
    "    # Save the scaled DataFrame\n",
    "    final_df.to_csv(scaled_output_file, index=False)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Apply the function to read, concatenate, and scale the time series data\n",
    "folder_path = 'time_series_data'\n",
    "output_file = 'output/final_time_series_data.csv'\n",
    "scaled_output_file = 'output/scaled_time_series_data.csv'\n",
    "scaler_output_file = 'output/scaler.pkl'\n",
    "scaled_dataframe = read_add_missing_concat_and_scale(folder_path, output_file, scaled_output_file, scaler_output_file)\n",
    "\n",
    "# Print the number of records in the scaled data\n",
    "print('Number of time series records: ', len(scaled_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931787da-b356-480d-9aeb-b09dd50e4484",
   "metadata": {},
   "source": [
    "# Process Data and Create 10-Year History\n",
    "This cell processes the data (both train and test) by creating a 10-year history for each row and one-hot encoding the 'Sector' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66add54f-6e5e-4959-ab16-587ad3b97890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174a1499c2c2492ba34a228d5e10c6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fb3df5c1954b3f8dfb906071e3854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Function to process data and create a 10-year history for each row\n",
    "def process_data(df, scaled_dataframe, policy_sectors, mode='train'):\n",
    "    processed_data = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Get time series data for the corresponding country\n",
    "        time_series_df = scaled_dataframe.loc[scaled_dataframe['country'] == row['Geography']]\n",
    "        \n",
    "        # Extract year from 'Last event in timeline'\n",
    "        last_event_year = pd.to_datetime(row['Last event in timeline']).year\n",
    "        year_range = list(range(last_event_year - 9, last_event_year + 1))\n",
    "        time_series_df = time_series_df[time_series_df['year'].isin(year_range)]\n",
    "        time_series_df = time_series_df.drop(columns=['year', 'country'])\n",
    "        \n",
    "        # Pad with zeros if fewer than 10 rows\n",
    "        if len(time_series_df) < 10:\n",
    "            padding_df = pd.DataFrame(np.zeros((10 - len(time_series_df), time_series_df.shape[1])), columns=time_series_df.columns)\n",
    "            time_series_df = pd.concat([padding_df, time_series_df], ignore_index=True)\n",
    "        \n",
    "        # Convert to NumPy array\n",
    "        time_series_np = time_series_df.to_numpy()\n",
    "        \n",
    "        # One-hot encode sectors\n",
    "        one_hot_encoding = np.zeros(len(policy_sectors), dtype=int)\n",
    "        for sector in row['Sector']:\n",
    "            if sector in policy_sectors:\n",
    "                one_hot_encoding[policy_sectors.index(sector)] = 1\n",
    "\n",
    "        # Append the processed data\n",
    "        if mode == 'test':\n",
    "            processed_data.append({\n",
    "                'doc_id': row['Document ID'],\n",
    "                'sector': row['Sector'],\n",
    "                'country': row['Geography'],\n",
    "                'anchor_summary': row['Family Summary'],\n",
    "                'positive_time_series': time_series_np.tolist(),\n",
    "                'positive_sector': one_hot_encoding.tolist(),\n",
    "            })\n",
    "        else:\n",
    "            processed_data.append({\n",
    "                'anchor_summary': row['Family Summary'],\n",
    "                'positive_time_series': time_series_np,\n",
    "                'positive_sector': one_hot_encoding.tolist(),\n",
    "            })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Process the training data\n",
    "processed_positive_data = process_data(train_df, scaled_dataframe, policy_sectors, mode='train')\n",
    "# Process the testing data\n",
    "processed_test_data = process_data(test_df, scaled_dataframe, policy_sectors, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd19955-9b6c-408e-84a0-c676637c5096",
   "metadata": {},
   "source": [
    "# Generate Negative Pairs and Save to JSONL\n",
    "This cell generates negative pairs for each positive pair and saves the combined data in JSONL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7a4611-d917-461a-bd17-e5a5bb05e54e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a8c36271d440fb9c198409cce61d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written incrementally to output/train_20.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to generate negative pairs and save to JSONL\n",
    "def generate_negative_pairs(train_df, scaled_dataframe, policy_sectors, positive_data, num_sample, jsonl_file_path='temp.jsonl', mode='train'):\n",
    "    sector_encoding_dict = {sector: np.eye(len(policy_sectors), dtype=int)[idx] for idx, sector in enumerate(policy_sectors)}\n",
    "    \n",
    "    with open(jsonl_file_path, 'w') as jsonl_file:\n",
    "        for pos_data in tqdm(positive_data, total=len(positive_data)):\n",
    "            anchor_summary = pos_data['anchor_summary']\n",
    "            pos_time_series = pos_data['positive_time_series']\n",
    "            pos_sector = pos_data['positive_sector']\n",
    "\n",
    "            filtered_train_df = train_df[train_df['Family Summary'] != anchor_summary]\n",
    "            negative_samples = filtered_train_df.sample(n=num_sample if mode == 'train' else 1)\n",
    "\n",
    "            for _, random_row in negative_samples.iterrows():\n",
    "                rand_summary = random_row['Family Summary']\n",
    "                time_series_df = scaled_dataframe.loc[scaled_dataframe['country'] == random_row['Geography']]\n",
    "                last_event_year = pd.to_datetime(random_row['Last event in timeline']).year\n",
    "                year_range = list(range(last_event_year - 9, last_event_year + 1))\n",
    "                time_series_df = time_series_df[time_series_df['year'].isin(year_range)].drop(columns=['year', 'country'])\n",
    "\n",
    "                if len(time_series_df) < 5:\n",
    "                    continue\n",
    "                \n",
    "                if len(time_series_df) < 10:\n",
    "                    padding_df = pd.DataFrame(np.zeros((10 - len(time_series_df), time_series_df.shape[1])), columns=time_series_df.columns)\n",
    "                    time_series_df = pd.concat([padding_df, time_series_df], ignore_index=True)\n",
    "                \n",
    "                time_series_np = time_series_df.to_numpy()\n",
    "                negative_sector = np.zeros(len(policy_sectors), dtype=int)\n",
    "                for sector in random_row['Sector']:\n",
    "                    if sector in sector_encoding_dict:\n",
    "                        negative_sector += sector_encoding_dict[sector]\n",
    "\n",
    "                combined_data = {\n",
    "                    'anchor_summary': anchor_summary,\n",
    "                    'positive_time_series': pos_time_series.tolist(),\n",
    "                    'positive_sector': pos_sector,\n",
    "                    'negative_time_series': time_series_np.tolist(),\n",
    "                    'negative_sector': negative_sector.tolist(),\n",
    "                }\n",
    "                jsonl_file.write(json.dumps(combined_data) + '\\n')\n",
    "\n",
    "    print(f\"Data written incrementally to {jsonl_file_path}\")\n",
    "\n",
    "# Generate negative pairs for train data and save to JSONL\n",
    "generate_negative_pairs(\n",
    "    train_df, \n",
    "    scaled_dataframe, \n",
    "    policy_sectors, \n",
    "    processed_positive_data, \n",
    "    num_sample=1, \n",
    "    jsonl_file_path='output/train_1.jsonl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35febe3d-103f-43a5-8eec-841fdc5f34ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ad96d7-b34f-4668-8728-83617823f667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('output/test.json', 'w') as file:\n",
    "    json.dump(processed_test_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23c50c-a7a6-46c4-b6a5-7a74880809aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
